*******************
Linear Mixed Models
*******************

Linear mixed models (LMMs) are a generalisation of linear models [#f1]_ to allow the
ouctome to be described as a summation of both fixed and random effects [#f2]_.
LMM inference is implemented by the :mod:`glimix_core.lmm` module and described here.

.. |n| replace:: :math:`n`
.. |m| replace:: :math:`m`
.. |c| replace:: :math:`c`
.. |d| replace:: :math:`d`
.. |k| replace:: :math:`k`
.. |r| replace:: :math:`r`

.. _lmm-intro:

Introduction
============

A LMM can be described as ::

    ğ² = Xğœ· + Gğ® + ğ›œ,

where ğ® âˆ¼ ğ“(ğŸ, vâ‚€I) is a
vector of random effects and \epsilonáµ¢ are iid Normal random variables
with zero-mean and variance vâ‚ each.
The outcome-vector is thus distributed according to ::

    ğ² âˆ¼ ğ“(Xğœ·, vâ‚€GGáµ€ + vâ‚I)

The :class:`.LMM` class provides a FastLMM [#f3]_
implementation to perform inference over the variance parameters
vâ‚€ and vâ‚ and over the vector
ğœ· of fixed-effect sizes.
An instance of this class is created by providing the outcome ``y``,
the covariates ``X``, and the covariance ``K`` via its economic eigendecomposition.
Here is an example:

.. doctest::

    >>> from numpy import ones
    >>> from numpy_sugar.linalg import economic_qs_linear
    >>>
    >>> from glimix_core.lmm import LMM
    >>>
    >>> G = [[1, 2], [3, -1], [1.1, 0.5], [0.5, -0.4]]
    >>> QS = economic_qs_linear(G)
    >>> X = ones((4, 1))
    >>> y = [-1, 2, 0.3, 0.5]
    >>> lmm = LMM(y, X, QS)
    >>> lmm.fit(verbose=False)
    >>> lmm.lml()  # doctest: +FLOAT_CMP
    -2.2726234086180557

The method :func:`.LMM.fit` is called to optimise the marginal
likelihood over the fixed-effect sizes ğœ· and over the
variances vâ‚€ and vâ‚.
The resulting values for the above inference are:

.. doctest::

    >>> lmm.beta[0]  # doctest: +FLOAT_CMP
    0.0664650291693258
    >>> lmm.v0  # doctest: +FLOAT_CMP
    0.33736446158226896
    >>> lmm.v1  # doctest: +FLOAT_CMP
    0.012503600451739165

We also provide :class:`.FastScanner`,
an approximated LMM implementation for performing an even faster
inference across several (millions, for example) covariates independently.
More detail about this approach is given in the `Association scan`_ section.

Multi-Trait
===========

This package also provides a variant of LMM that models multiple outcomes (or traits) of
the same set of samples.
Let p be the number of traits.
The outcome matrix Y is the concatenation of p vectors::

    Y = [ğ²â‚€ ğ²â‚ ... ğ²â‚š].

The mean definition will involve three matrices::

    M = (A âŠ— F) vec(B),

where vec(Â·) stacks the columns of the input matrix into a single-column matrix.
B is a cÃ—p matrix of effect sizes for c being the number of covariates.
A is a pÃ—p design matrix that determines the covariance between the traits over the mean
vector.
F is a nÃ—p design matrix of covariates.

The covariance matrix will be::

    K = Câ‚€ âŠ— GGáµ€ + Câ‚ âŠ— I.

Câ‚€ and Câ‚ are pÃ—p symmetric matrices whose values will be optimized.
GGáµ€ gives the covariance between samples, while (Câ‚€ âŠ— GGáµ€) gives the covariance between
samples when traits are taken into account.

Putting the outcome, mean, and covariance-matrix together, we have the distribution ::

    vec(Y) ~ N((A âŠ— F) vec(B), K = Câ‚€ âŠ— GGáµ€ + Câ‚ âŠ— I).

The parameters of the multi-trait LMM to be fit via maximum likelihood are the matrices
B, Câ‚€, and Câ‚.

.. doctest::

    >>> from numpy.random import RandomState
    >>> from glimix_core.lmm import Kron2Sum
    >>>
    >>> random = RandomState(0)
    >>> n = 5
    >>> p = 2
    >>> c = 3
    >>> Y = random.randn(n, p)
    >>> A = random.randn(p, p)
    >>> A = A @ A.T
    >>> F = random.randn(n, c)
    >>> G = random.randn(n, 4)
    >>>
    >>> mlmm = Kron2Sum(Y, A, F, G)
    >>> mlmm.fit(verbose=False)
    >>> mlmm.lml()  # doctest: +FLOAT_CMP
    -5.666702537532974
    >>> mlmm.B  # doctest: +FLOAT_CMP
    array([[-0.17170011,  0.45565163],
           [ 0.57532031, -0.86070064],
           [ 0.21050686, -0.02573517]])
    >>> mlmm.cov.C0.value()  # doctest: +FLOAT_CMP
    array([[ 0.01598945, -0.04374046],
           [-0.04374046,  0.11965561]])
    >>> mlmm.cov.C1.value()  # doctest: +FLOAT_CMP
    array([[1.2051213 , 1.49844327],
           [1.49844327, 1.86319675]])

We also provide :class:`.KronFastScanner` for performing an even faster
inference across several (millions, for example) covariates independently.
Please, follow the next section for details.

Association scan
================

Let X be a nÃ—c matrix, Mâ±¼ a nÃ—mâ±¼ matrix for the j-th candidate set, and ğ² an array of
outcome::

    ğ² âˆ¼ ğ“(Xğœ·â±¼ + Mâ±¼ğ›‚â±¼, sâ±¼(vâ‚€GGáµ€ + vâ‚I))

The parameters ğœ·â±¼, ğ›‚â±¼, and sâ±¼ are fit via maximum likelihood, while the remaining
parameters vâ‚€ and vâ‚ are held fixed. The vâ‚€ and vâ‚ values are first found by applying
:class:`.LMM`.

.. doctest::

    >>> scanner = lmm.get_fast_scanner()
    >>> M = [[1.5, 0.1], [-0.2, 0.4], [0.0, 1.0], [-3.4, 0.6]]
    >>> lml, eff0, eff1, scale = scanner.scan(M)
    >>> lml  # doctest: +FLOAT_CMP
    -0.7322976913217882
    >>> print(eff0)  # doctest: +FLOAT_CMP
    [-0.42323051]
    >>> print(eff1)  # doctest: +FLOAT_CMP
    [-0.05913491  0.37079162]
    >>> scale  # doctest: +FLOAT_CMP
    0.4629376687687552

For the null case (i.e., when there is not candidate set Mâ±¼), the log of the marginal
likelihood and the values of ğœ· and s can be found as follows.

.. doctest::

    >>> scanner.null_lml()  # doctest: +FLOAT_CMP
    -2.272623408618055
    >>> scanner.null_effsizes()  # doctest: +FLOAT_CMP
    array([0.06646503])
    >>> scanner.null_scale()  # doctest: +FLOAT_CMP
    1.0

We also provide a fast scanner for the multi-trait case, :class:`.KronFastScanner`.
Its model is given by ::

    vec(Y) âˆ¼ ğ“(vec(Y) | (A âŠ— F)vec(ğš©â±¼) + (Aâ±¼ âŠ— Fâ±¼)vec(ğš¨â±¼), sâ±¼(Câ‚€ âŠ— GGáµ€ + Câ‚ âŠ— I)).

As before, the parameters Câ‚€ and Câ‚ are set to the values found by :class:`.Kron2Sum`.
A candidate set is defined by providing the matrices Aâ±¼ and Fâ±¼.
The parameters ğš©â±¼, ğš¨â±¼, and sâ±¼ are found via maximum likelihood.

.. doctest::

    >>> mscanner = mlmm.get_fast_scanner()
    >>> A = random.randn(2, 5)
    >>> F = random.randn(5, 3)
    >>> lml, eff0, eff1, scale = mscanner.scan(A, F)
    >>> lml
    81.87502470339223
    >>> eff0
    array([[ 0.01482133,  0.45189275],
           [ 0.43706748, -0.71162517],
           [ 0.52595486, -1.59740035]])
    >>> eff1
    array([[ 0.03868156, -0.77199913, -0.09643554, -0.53973775,  1.03149564],
           [ 0.05780863, -0.24744739, -0.11882984, -0.19331759,  0.74964805],
           [ 0.01051071, -1.61751886, -0.0654883 , -1.09931899,  1.51034738]])
    >>> scale
    2.220446049250313e-16

API
===

.. currentmodule:: glimix_core.lmm

.. autosummary::
  :toctree: _autosummary
  :template: class.rst

  FastScanner
  Kron2Sum
  KronFastScanner
  LMM

Implementation
==============

Single-trait
------------

The LMM model :eq:`lmm1` can be equivalently written as

.. math::

    ğ² âˆ¼ ğ“\Big(~ Xğœ·;~
      s \big(
        (1-ğ›¿)
          K +
        ğ›¿ I
      \big)
    ~\Big),

and we thus have vâ‚€ = s (1 - ğ›¿) and vâ‚ = s ğ›¿.
Consider the economic eigendecomposition of K:

.. math::

    \overbrace{[\mathrm Qâ‚€ \quad \mathrm Qâ‚]}^{\mathrm Q}
        \overbrace{\left[\begin{array}{cc}
            \mathrm Sâ‚€ & ğŸ\\
            ğŸ & ğŸ
        \end{array}\right]}^{\mathrm S}
    \left[\begin{array}{c}
        \mathrm Qâ‚€áµ€ \\
        \mathrm Qâ‚áµ€
    \end{array}\right] = K

and let

.. math::

    \mathrm D = \left[
        \begin{array}{cc}
          (1-ğ›¿)\mathrm Sâ‚€ + ğ›¿I_r & ğŸ\\
          ğŸ & ğ›¿I_{n-r}
        \end{array}
        \right].

We thus have

.. math::

    ((1-ğ›¿)K + ğ›¿I)â»Â¹ =
        \mathrm Q \mathrm Dâ»Â¹
        \mathrm Qáµ€.

A diagonal covariance-matrix can then be used to define an equivalent
marginal likelihood:

.. math::

    ğ“\left(\mathrm Qáµ€ ğ² ~|~
               \mathrm Qáµ€ Xğœ·,~
               s \mathrm D \right).


Taking the logarithm and expanding it gives us

.. math::

   log p(ğ²) &=
       -\frac{n}{2} log 2\pi - \frac{n}{2} log s
           - \frac{1}{2}log|\mathrm D|\\
       &- \frac{1}{2} (\mathrm Qáµ€ğ²)áµ€ sâ»Â¹
           \mathrm Dâ»Â¹(\mathrm Qáµ€ ğ²)\\
       &+ (\mathrm Qáµ€ğ²)áµ€
           sâ»Â¹ \mathrm Dâ»Â¹
           (\mathrm Qáµ€ X ğœ·)\\
       &- \frac{1}{2} (\mathrm Qáµ€
           X ğœ·)áµ€ sâ»Â¹ \mathrm Dâ»Â¹
           (\mathrm Qáµ€ X ğœ·).

Setting the derivative of log(p(ğ²)) over effect sizes equal
to zero leads to solutions ğœ·^* from equation

.. math::

   (\mathrm Qáµ€X ğœ·^*)áµ€
       \mathrm Dâ»Â¹ (\mathrm Qáµ€ X) =
       (\mathrm Qáµ€ğ²)áµ€\mathrm Dâ»Â¹
       (\mathrm Qáµ€X).

Replacing it back to the log of the marginal likelihood gives us the simpler
formulae

.. math::

   log p(ğ²) &=
       -\frac{n}{2} log 2\pi - \frac{n}{2} log s
           - \frac{1}{2}log|\mathrm D|\\
       & +\frac{1}{2} (\mathrm Qáµ€ğ²)áµ€ sâ»Â¹
           \mathrm Dâ»Â¹\mathrm Qáµ€
           (Xğœ·^* - ğ²).


In the extreme case where ğœ·^* is such that
ğ² = Xğœ·^*, the maximum is attained
as :math:`s \rightarrow 0`.

Setting the derivative of :math:`log p(ğ²; ğœ·^*)` over
scale equal to zero leads to the maximum

.. math::

   s^* = nâ»Â¹
       (\mathrm Qáµ€ğ²)áµ€
           \mathrm Dâ»Â¹\mathrm Qáµ€
           (ğ² - Xğœ·^*).


We offer the possibility to use either :math:`s^*` found via the
above equation or a scale defined by the user.
In the first case we have a further simplification of the log of the marginal
likelihood:

.. math::

   log p(ğ²; ğœ·^*, s^*) &=
       -\frac{n}{2} log 2\pi - \frac{n}{2} log s^*
           - \frac{1}{2}log|\mathrm D| - \frac{n}{2}\\
           &= log ğ“(\text{Diag}(\sqrt{s^*\mathrm D})
               ~|~ ğŸ, s^*\mathrm D).

.. _mtlmm-impl:

Uncorrelated multi-trait
------------------------

The extension to multiple traits becomes easy under the assumption that the traits are
uncorrelated, as assumed in this section.
Let m be the number of traits.
We stack all the different traits into

.. math::

    ğ² = \text{vec}\left(\left[ ğ²â‚€ ~ ğ²â‚ ~\cdots~ ğ²_m
        \right] \right)

Similarly, we have the covariates, fixed-effect sizes, and the assembled covariance
matrix :math:`\tilde{K}` as

.. math::

    X = \text{vec}\left(\left[ Xâ‚€ ~ Xâ‚ ~\cdots~ X_m
        \right] \right),

.. math::

    ğœ· = \text{vec}\left(
        \left[
            ğœ·â‚€ ~ ğœ·â‚ ~\cdots~ ğœ·_m
        \right]
    \right),

and

.. math::

    \tilde{K} =
        \left[
            \begin{array}{ccc}
                K & ğŸ & \cdots \\
                \vdots    & \ddots    &        \\
                ğŸ &           & K
            \end{array}
        \right],

where K is repeated \|m\| times in :math:`\tilde{K}`.
We thus consider the model

.. math::
    :label: mtlmm1

    ğ² âˆ¼ ğ“\Big(~
        Xğœ·;~
        vâ‚€ \tilde{K} + vâ‚ I_{nm}
    ~\Big),

which is the model :eq:`lmm1` with multi-trait structure and uncorrelated traits.

We use the fact that the eigendecomposition of :math:`\tilde{K}` can be
computed as fast as the eigendecomposition of K:

.. math::

    \overbrace{[\mathrm Q ~ \cdots ~ \mathrm Q]}^{\tilde{\mathrm Q}}
        \overbrace{\left[\begin{array}{ccc}
            \mathrm S & ğŸ & \cdots \\
            \vdots    & \ddots    &   \\
            ğŸ &           & \mathrm S
        \end{array}\right]}^{\tilde{\mathrm S}}
    \left[\begin{array}{c}
        \mathrm Qáµ€ \\
        \vdots \\
        \mathrm Qáµ€
    \end{array}\right] = \tilde{K}.

Let

.. math::

    \tilde{\mathrm D} = \left[
        \begin{array}{cc}
          (1-ğ›¿)\tilde{\mathrm S}â‚€ + ğ›¿I_r & ğŸ\\
          ğŸ & ğ›¿I_{n-r}
        \end{array}
        \right].

We thus have

.. math::

    ((1-ğ›¿)\tilde{K} + ğ›¿I_{nm})â»Â¹ =
            \left[\begin{array}{ccc}
                \mathrm Q\mathrm Dâ»Â¹\mathrm Qáµ€ & ğŸ & \cdots\\
                \vdots    & \ddots    & \\
                ğŸ &           & \mathrm Q\mathrm Dâ»Â¹\mathrm Qáµ€
            \end{array}\right]

A diagonal covariance-matrix can then be used to define an equivalent marginal
likelihood:

.. math::

    ğ“\left(\tilde{\mathrm Q}áµ€ ğ² ~|~
               \tilde{\mathrm Q}áµ€
               Xğœ·
               ,~
               s\left[\begin{array}{ccc}
                    \mathrm D & ğŸ & \cdots\\
                    \vdots    & \ddots    & \\
                    ğŸ &           & \mathrm D
                \end{array}\right]
               \right).

The optimal effect sizes are solutions to the equation

.. math::

   \left[(\mathrm Qáµ€Xáµ¢ğœ·áµ¢^*)áµ€
       \mathrm Dâ»Â¹ (\mathrm Qáµ€ Xáµ¢)\right] =
       \left[(\mathrm Qáµ€ğ²áµ¢)áµ€\mathrm Dâ»Â¹
       (\mathrm Qáµ€Xáµ¢)\right],

for :math:`i \in \{1, \dots, m\}`.
Setting the derivative of log(p(ğ²; ğœ·^*)) over scale equal
to zero leads to the maximum

.. math::

   s^* = (nm)â»Â¹
       \left[(\mathrm Qáµ€ğ²áµ¢)áµ€
           \mathrm Dâ»Â¹\right]\left[\mathrm Qáµ€
           (ğ²áµ¢ - Xáµ¢ğœ·áµ¢^*)\right].


Using the above, optimal scale leads to a further simplification of the log of the
marginal likelihood:

.. math::

   log p(ğ²; ğœ·^*, s^*) &=
       -\frac{nm}{2} log 2\pi - \frac{nm}{2} log s^*
           - \frac{1}{2}log|\tilde{\mathrm D}| - \frac{nm}{2}\\
           &= log ğ“(\text{Diag}(\sqrt{s^*\tilde{\mathrm D}})
               ~|~ ğŸ, s^*\tilde{\mathrm D}).

Multi-trait
-----------

Please, refer to :class:`glimix_core.lmm.Kron2Sum`.

.. rubric:: References

.. [#f1] Wikipedia contributors. (2018, May 22). Linear model. In Wikipedia, The Free
         Encyclopedia. Retrieved 16:00, August 5, 2018, from
         https://en.wikipedia.org/w/index.php?title=Linear_model&oldid=842479751.

.. [#f2] Introduction to linear mixed models. UCLA: Institute for Digital Research and
         Education. Retrieved from August 5, 2018, from
         https://stats.idre.ucla.edu/other/mult-pkg/introduction-to-linear-mixed-models/.

.. [#f3] Lippert, Christoph, Listgarten, Jennifer, Liu, Ying, Kadie, Carl M,
         Davidson, Robert I & Heckerman, David (2011). FaST linear mixed
         models for genome-wide association studies. Nature methods, 8,
         833-835.
