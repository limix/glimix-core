import warnings
from functools import lru_cache, reduce

from numpy import asarray, asfortranarray, diagonal, eye, kron, sqrt, tensordot
from numpy.linalg import matrix_rank, slogdet, solve

from glimix_core._util import log2pi, unvec, vec
from glimix_core.cov import Kron2SumCov
from glimix_core.mean import KronMean
from numpy_sugar.linalg import ddot
from optimix import Function


class RKron2Sum(Function):
    """
    LMM for multiple traits.

    Let n, c, and p be the number of samples, covariates, and traits, respectively.
    The outcome variable Y is a nÃ—p matrix distributed according to::

        vec(Y) ~ N((A âŠ— F) vec(B), K = Câ‚€ âŠ— GGáµ— + Câ‚ âŠ— I).

    A and F are design matrices of dimensions pÃ—p and nÃ—c provided by the user,
    where F is the usual matrix of covariates commonly used in single-trait models.
    B is a cÃ—p matrix of fixed-effect sizes per trait.
    G is a nÃ—r matrix provided by the user and I is a nÃ—n identity matrices.
    Câ‚€ and Câ‚ are both symmetric matrices of dimensions pÃ—p, for which Câ‚ is
    guaranteed by our implementation to be of full rank.
    The parameters of this model are the matrices B, Câ‚€, and Câ‚.

    For implementation purpose, we make use of the following definitions:

    - M = A âŠ— F
    - H = Máµ€Kâ»Â¹M
    - Yâ‚“ = Lâ‚“Y
    - Yâ‚• = Yâ‚“Lâ‚•áµ€
    - Mâ‚“ = Lâ‚“F
    - Mâ‚• = (Lâ‚•A) âŠ— Mâ‚“
    - mâ‚• = Mâ‚•vec(B)

    where Lâ‚“ and Lâ‚• are defined in :class:`glimix_core.cov.Kron2SumCov`.
    """

    def __init__(self, Y, A, F, G, rank=1):
        """
        Constructor.

        Parameters
        ----------
        Y : (n, p) array_like
            Outcome matrix.
        A : (n, n) array_like
            Trait-by-trait design matrix.
        F : (n, c) array_like
            Covariates design matrix.
        G : (n, r) array_like
            Matrix G from the GGáµ— term.
        rank : optional, int
            Maximum rank of matrix Câ‚€. Defaults to ``1``.
        """
        Y = asfortranarray(Y, float)
        yrank = matrix_rank(Y)
        if Y.shape[1] > yrank:
            warnings.warn(
                f"Y is not full column rank: rank(Y)={yrank}. "
                + "Convergence might be problematic.",
                UserWarning,
            )

        A = asarray(A, float)
        F = asarray(F, float)
        G = asarray(G, float)
        self._Y = Y
        # self._y = Y.ravel(order="F")
        self._cov = Kron2SumCov(G, Y.shape[1], rank)
        self._mean = KronMean(A, F)
        self._Yx = self._cov.Lx @ Y
        self._Mx = self._cov.Lx @ self._mean.F
        # self._Yxe = self._cov._Lxe @ Y
        # self._Mxe = self._cov._Lxe @ self._mean.F
        self._cache = {"terms": None}
        self._cov.listen(self._parameters_update)
        composite = [("C0", self._cov.C0), ("C1", self._cov.C1)]
        Function.__init__(self, "Kron2Sum", composite=composite)

    def _parameters_update(self):
        self._cache["terms"] = None

    @property
    def GY(self):
        return self._cov._Ge.T @ self._Y

    @property
    def GYGY(self):
        return self.GY ** 2

    @property
    def GG(self):
        return self._cov._Ge.T @ self._cov._Ge

    @property
    def GGGG(self):
        return self.GG @ self.GG

    @property
    def _terms(self):
        if self._cache["terms"] is not None:
            return self._cache["terms"]

        Lh = self._cov.Lh
        D = self._cov.D
        yh = vec(self._Yx @ Lh.T)
        # yhe = vec(self._Yxe @ Lh.T)
        yl = D * yh
        A = self._mean.A
        Mh = kron(Lh @ A, self._Mx)
        # Mhe = kron(Lh @ A, self._Mxe)
        Ml = ddot(D, Mh)

        # H = Máµ—Kâ»Â¹M.
        H = Mh.T @ Ml

        # ğ¦ = Mğ›ƒ for ğ›ƒ = Hâ»Â¹Máµ—Kâ»Â¹ğ² and H = Máµ—Kâ»Â¹M.
        # ğ›ƒ = Hâ»Â¹Máµ—â‚•Dğ²â‚—
        b = solve(H, Mh.T @ yl)
        B = unvec(b, (self.ncovariates, -1))
        self._mean.B = B

        mh = Mh @ b
        # mhe = Mhe @ b
        ml = D * mh

        ldetH = slogdet(H)
        if ldetH[0] != 1.0:
            raise ValueError("The determinant of H should be positive.")
        ldetH = ldetH[1]

        # breakpoint()
        L0 = self._cov.C0.L
        S, U = self._cov.C1.eigh()
        S = 1 / sqrt(S)
        US = ddot(U, S)
        X = kron(self._cov.C0.L, self._cov._G)
        R = (
            kron(self._cov.C1.L, eye(self._cov._G.shape[0]))
            @ kron(self._cov.C1.L, eye(self._cov._G.shape[0])).T
        )
        Y = self._Y
        Ge = self._cov._Ge
        K = X @ X.T + R
        W = kron(ddot(U, S), eye(Ge.shape[0]))
        Ri = W @ W.T
        # Z = eye(Ge.shape[1]) + X.T @ solve(R, X)
        # Ki = Ri - Ri @ X @ solve(Z, X.T @ Ri)
        y = vec(self._Y)
        # yKiy = y.T @ Ri @ y - y.T @ Ri @ X @ solve(Z, X.T @ Ri @ y)
        WY = Y @ US
        # yRiy = vec(WY).T @ vec(WY)
        F = self._mean.F
        A = self._mean.A
        WM = kron(US.T @ A, F)
        WB = F @ B @ A.T @ US
        G = self._cov._G
        # WX = kron(US.T @ L0, G)
        WX = kron(US.T @ L0, Ge)
        # Z0 = kron(L0.T @ ddot(U, S * S) @ U.T @ L0, G.T @ G)
        Z0 = kron(L0.T @ ddot(U, S * S) @ U.T @ L0, Ge.T @ Ge)
        # Z = eye(G.shape[1]) + Z0
        # breakpoint()
        Z = eye(Ge.shape[1]) + Z0
        yKiy = vec(WY).T @ vec(WY) - vec(WY).T @ WX @ solve(Z, WX.T @ vec(WY))
        MKiM = WM.T @ WM - WM.T @ WX @ solve(Z, WX.T @ WM)
        # MRiM = kron(A.T @ ddot(U, S ** 2) @ U.T @ A, F.T @ F)
        b = solve(MKiM, WM.T @ vec(WY) - WM.T @ WX @ solve(Z, WX.T @ vec(WY)))
        B = unvec(b, (self.ncovariates, -1))
        Wm = WM @ b

        # w = ddot(U, S)
        # WTY = self._Y @ w
        # wA = w @ self._mean.A
        # WTM = (wA, self._mean.F)
        # WTm = vec(self._mean.F @ (B @ wA.T))
        # # XX^t = kron(C0, GG^t)
        # XTW = (L0.T @ w, self._cov._G.T)
        # XTWWTY = self._cov._G.T @ WTY @ w.T @ L0

        # # Z = (L0.T @ w.T, self._cov._G.T) @ (L0 @ w, self._cov._G)
        # Z = kron(L0.T @ w.T @ w @ L0, self._cov._G.T @ self._cov._G)
        # Z += eye(Z.shape[0])

        # r0 = vec(WTY.T) @ vec(WTY) - vec(XTWWTY).T @ solve(Z, vec(XTWWTY))
        # r1 = vec(self._Y).T @ solve(self._cov.value(), vec(self._Y))

        # self._y.T

        self._cache["terms"] = {
            "yh": yh,
            "yl": yl,
            "Mh": Mh,
            "Ml": Ml,
            "mh": mh,
            "ml": ml,
            "ldetH": ldetH,
            "H": H,
            "b": b,
            "Z": Z,
            "WM": WM,
            "WY": WY,
            "WX": WX,
            "W": W,
            "R": R,
            "K": K,
            "B": B,
            "Wm": Wm,
            "Ri": Ri,
            "X": X,
            "US": US
            # "yhe": yhe,
            # "Mhe": Mhe,
            # "mhe": mhe,
        }
        return self._cache["terms"]

    @property
    def mean(self):
        """
        Mean ğ¦ = (A âŠ— F) vec(B).

        Returns
        -------
        mean : KronMean
        """
        return self._mean

    @property
    def cov(self):
        """
        Covariance K = Câ‚€ âŠ— GGáµ— + Câ‚ âŠ— I.

        Returns
        -------
        covariance : Kron2SumCov
        """
        return self._cov

    @property
    def nsamples(self):
        """
        Number of samples, n.
        """
        return self._Y.shape[0]

    @property
    def ntraits(self):
        """
        Number of traits, p.
        """
        return self._Y.shape[1]

    @property
    def ncovariates(self):
        """
        Number of covariates, c.
        """
        return self._mean.F.shape[1]

    def value(self):
        return self.lml()

    def gradient(self):
        return self.lml_gradient()

    @property
    @lru_cache(maxsize=None)
    def _logdet_MM(self):
        M = self._mean.AF
        ldet = slogdet(M.T @ M)
        if ldet[0] != 1.0:
            raise ValueError("The determinant of Máµ€M should be positive.")
        return ldet[1]

    def lml(self):
        r"""
        Log of the marginal likelihood.

        Let ğ² = vec(Y), M = AâŠ—F, and H = Máµ€Kâ»Â¹M. The restricted log of the marginal
        likelihood is given by [R07]_::

            2â‹…log(p(ğ²)) = -(nâ‹…p - câ‹…p) log(2Ï€) + log(ï½œMáµ—Mï½œ) - log(ï½œKï½œ) - log(ï½œHï½œ)
                - (ğ²-ğ¦)áµ— Kâ»Â¹ (ğ²-ğ¦),

        where ğ¦ = Mğ›ƒ for ğ›ƒ = Hâ»Â¹Máµ—Kâ»Â¹ğ² and H = Máµ—Kâ»Â¹M.

        For implementation purpose, let X = (Lâ‚€ âŠ— G) and R = (Lâ‚ âŠ— I)(Lâ‚ âŠ— I)áµ€.
        The covariance can be written as::

            K = XXáµ€ + R.

        From the Woodbury matrix identity, we have

            ğ²áµ€Kâ»Â¹ğ² = ğ²áµ€Râ»Â¹ğ² - ğ²áµ€Râ»Â¹XZâ»Â¹Xáµ€Râ»Â¹ğ²,

        where Z = I + Xáµ€Râ»Â¹X. Note that Râ»Â¹ = (Uâ‚Sâ‚â»Â½ âŠ— I)(Uâ‚Sâ‚â»Â½ âŠ— I)áµ€ and ::

            Xáµ€Râ»Â¹ğ² = (Lâ‚€áµ€Uâ‚Sâ‚â»Â¹Uâ‚áµ€ âŠ— Gáµ€)ğ² = vec(Gáµ€YUâ‚Sâ‚â»Â¹Uâ‚áµ€Lâ‚€).

        The term Gáµ€Y can be calculated only once and it will form a rÃ—p matrix. We
        similarly have ::

            Xáµ€Râ»Â¹M = (Lâ‚€áµ€Uâ‚Sâ‚â»Â¹Uâ‚áµ€A âŠ— Gáµ€F),

        for which Gáµ€F is pre-computed.

        The log-determinant of the covariance matrix is given by

            log(ï½œKï½œ) = log(ï½œZï½œ) - log(ï½œRâ»Â¹ï½œ) = log(ï½œZï½œ) - 2Â·nÂ·log(ï½œUâ‚Sâ‚â»Â½ï½œ).

        The log of the marginal likelihood can be rewritten as::

            2â‹…log(p(ğ²)) = -(nâ‹…p - câ‹…p) log(2Ï€) + log(ï½œMáµ—Mï½œ)
            - log(ï½œZï½œ) + 2Â·nÂ·log(ï½œUâ‚Sâ‚â»Â½ï½œ)
            - log(ï½œMáµ€Râ»Â¹M - Máµ€Râ»Â¹XZâ»Â¹Xáµ€Râ»Â¹Mï½œ)
            - ğ²áµ€Râ»Â¹ğ² + ğ²áµ€Râ»Â¹XZâ»Â¹Xáµ€Râ»Â¹ğ²
            - ğ¦áµ€Râ»Â¹ğ¦ + ğ¦áµ€Râ»Â¹XZâ»Â¹Xáµ€Râ»Â¹ğ¦
            + 2â‹…ğ²áµ€Râ»Â¹ğ¦ - 2â‹…ğ²áµ€Râ»Â¹XZâ»Â¹Xáµ€Râ»Â¹ğ¦.

        Returns
        -------
        lml : float
            Log of the marginal likelihood.

        References
        ----------
        .. [R07] LaMotte, L. R. (2007). A direct derivation of the REML likelihood
           function. Statistical Papers, 48(2), 321-327.
        """
        np = self.nsamples * self.ntraits
        cp = self.ncovariates * self.ntraits
        terms = self._terms
        Z = terms["Z"]
        WY = terms["WY"]
        WX = terms["WX"]
        WM = terms["WM"]
        Wm = terms["Wm"]
        US = terms["US"]
        cov_logdet = slogdet(Z)[1] - 2 * slogdet(US)[1] * self.nsamples
        lml = -(np - cp) * log2pi + self._logdet_MM - cov_logdet

        MKiM = WM.T @ WM - WM.T @ WX @ solve(Z, WX.T @ WM)
        lml -= slogdet(MKiM)[1]

        yKiy = vec(WY).T @ vec(WY) - vec(WY).T @ WX @ solve(Z, WX.T @ vec(WY))
        mKiy = vec(Wm).T @ vec(WY) - vec(Wm).T @ WX @ solve(Z, WX.T @ vec(WY))
        mKim = vec(Wm).T @ vec(Wm) - vec(Wm).T @ WX @ solve(Z, WX.T @ vec(Wm))
        lml += -yKiy - mKim + 2 * mKiy

        return lml / 2

    def lml_gradient(self):
        """
        Gradient of the log of the marginal likelihood.

        Let ğ² = vec(Y), ğ•‚ = Kâ»Â¹âˆ‚(K)Kâ»Â¹, and H = Máµ€Kâ»Â¹M. The gradient is given by::

            2â‹…âˆ‚log(p(ğ²)) = -tr(Kâ»Â¹âˆ‚K) - tr(Hâ»Â¹âˆ‚H) + ğ²áµ€ğ•‚ğ² - ğ¦áµ€ğ•‚(2â‹…ğ²-ğ¦)
                - 2â‹…(ğ¦-ğ²)áµ€Kâ»Â¹âˆ‚(ğ¦).

        For implementation purposes, we use Woodbury matrix identity to write

            ğ²áµ€ğ•‚ğ² = ğ²áµ€ğ“¡ğ² - 2â‹…ğ²áµ€ğ“¡XZâ»Â¹Xáµ€Râ»Â¹ğ² + ğ²áµ€Râ»Â¹XZâ»Â¹Xáµ€ğ“¡XZâ»Â¹Xáµ€Râ»Â¹ğ².

        where ğ“¡ = Râ»Â¹âˆ‚(K)Râ»Â¹. We compute the above equation as follows::

            ğ²áµ€ğ“¡ğ² = ğ²áµ€(Uâ‚Sâ‚â»Â¹Uâ‚áµ€ âŠ— I)(âˆ‚Câ‚€ âŠ— GGáµ€)(Uâ‚Sâ‚â»Â¹Uâ‚áµ€ âŠ— I)ğ²
                  = ğ²áµ€(Uâ‚Sâ‚â»Â¹Uâ‚áµ€âˆ‚Câ‚€ âŠ— G)(Uâ‚Sâ‚â»Â¹Uâ‚áµ€ âŠ— Gáµ€)ğ²
                  = vec(Gáµ€YUâ‚Sâ‚â»Â¹Uâ‚áµ€âˆ‚Câ‚€)áµ€vec(Gáµ€YUâ‚Sâ‚â»Â¹Uâ‚áµ€),

        when the derivative is over the parameters of Câ‚€. Otherwise, we have

            ğ²áµ€ğ“¡ğ² = vec(YUâ‚Sâ‚â»Â¹Uâ‚áµ€âˆ‚Câ‚)áµ€vec(YUâ‚Sâ‚â»Â¹Uâ‚áµ€).

        We have

            Xáµ€ğ“¡ğ² = (Lâ‚€ âŠ— G)áµ€(Uâ‚Sâ‚â»Â¹Uâ‚áµ€ âŠ— I)(âˆ‚Câ‚€ âŠ— GGáµ€)(Uâ‚Sâ‚â»Â¹Uâ‚áµ€ âŠ— I)ğ²
                  = (Lâ‚€áµ€Uâ‚Sâ‚â»Â¹Uâ‚áµ€âˆ‚Câ‚€ âŠ— Gáµ€G) vec(Gáµ€YUâ‚Sâ‚â»Â¹Uâ‚áµ€)
                  = vec(Gáµ€GGáµ€YUâ‚Sâ‚â»Â¹Uâ‚áµ€âˆ‚Câ‚€Uâ‚Sâ‚â»Â¹Uâ‚áµ€Lâ‚€),

        when the derivative is over the parameters of Câ‚€. Otherwise, we have

            Xáµ€ğ“¡ğ² = vec(Gáµ€YUâ‚Sâ‚â»Â¹Uâ‚áµ€âˆ‚Câ‚Uâ‚Sâ‚â»Â¹Uâ‚áµ€Lâ‚€).

        We also have

            Xáµ€ğ“¡X = (Lâ‚€áµ€Uâ‚Sâ‚â»Â¹Uâ‚áµ€âˆ‚Câ‚€Uâ‚Sâ‚â»Â¹Uâ‚áµ€Lâ‚€) âŠ— (Gáµ€GGáµ€G),

        when the derivative is over the parameters of Câ‚€. Otherwise, we have

            Xáµ€ğ“¡X = (Lâ‚€áµ€Uâ‚Sâ‚â»Â¹Uâ‚áµ€âˆ‚Câ‚Uâ‚Sâ‚â»Â¹Uâ‚áµ€Lâ‚€) âŠ— (Gáµ€G).

        Returns
        -------
        C0.Lu : ndarray
            Gradient of the log of the marginal likelihood over Câ‚€ parameters.
        C1.Lu : ndarray
            Gradient of the log of the marginal likelihood over Câ‚ parameters.
        """

        def _dot(a, b):
            r = tensordot(a, b, axes=([1], [0]))
            if a.ndim > b.ndim:
                return r.transpose([0, 2, 1])
            return r

        def dot(*args):
            return reduce(_dot, args)

        def _sum(a):
            return a.sum(axis=(0, 1))

        terms = self._terms
        LdKLy = self._cov.LdKL_dot(terms["yl"])
        LdKLm = self._cov.LdKL_dot(terms["ml"])

        R = terms["R"]
        Ri = terms["Ri"]
        Riy = solve(terms["R"], vec(self._Y))
        r = Riy.T @ self._cov.gradient()["C0.Lu"][..., 0] @ Riy
        L0 = self._cov.C0.L
        L1 = self._cov.C1.L
        S, U = self._cov.C1.eigh()
        S = 1 / sqrt(S)
        US = ddot(U, S)
        G = self._cov.G
        Y = self._Y
        y = vec(Y)
        X = terms["X"]
        W = terms["W"]
        Ri = terms["Ri"]
        Z = terms["Z"]
        Ge = self._cov._Ge
        # X = kron(self._cov.C0.L, self._cov._G)
        # W = kron(ddot(U, S), eye(Ge.shape[0]))
        # Ri = W @ W.T

        XRiy = vec(Ge.T @ Y @ US @ US.T @ L0)

        M = self._mean.AF
        m = unvec(M @ vec(terms["B"]), (-1, self.ntraits))
        Gm = Ge.T @ m
        XRim = vec(Ge.T @ m @ US @ US.T @ L0)

        varnames = ["C0.Lu", "C1.Lu"]
        LdKLM = self._cov.LdKL_dot(terms["Ml"])
        dH = {n: -dot(terms["Ml"].T, LdKLM[n]).transpose([2, 0, 1]) for n in varnames}

        left = {n: (dH[n] @ terms["b"]).T for n in varnames}
        right = {n: terms["Ml"].T @ LdKLy[n] for n in varnames}
        db = {n: -solve(terms["H"], left[n] + right[n]) for n in varnames}

        grad = {}
        dmh = {n: terms["Mh"] @ db[n] for n in varnames}
        ld_grad = self._cov.logdet_gradient()
        ZiXRiy = solve(Z, XRiy)
        ZiXRim = solve(Z, XRim)
        for var in varnames:
            grad[var] = -ld_grad[var]

        SUL0 = US.T @ L0
        dC0 = self._cov.C0.gradient()["Lu"]
        SUdC0US = dot(US.T, dC0, US)
        dC1 = self._cov.C1.gradient()["Lu"]
        SUdC1US = dot(US.T, dC1, US)

        YUS = Y @ US
        GYUS = self.GY @ US
        GmUS = Gm @ US
        mUS = m @ US

        GG = self.GG
        GGGG = self.GGGG

        # Xáµ€ğ“¡X = (Lâ‚€áµ€Uâ‚Sâ‚â»Â¹Uâ‚áµ€âˆ‚Câ‚€Uâ‚Sâ‚â»Â¹Uâ‚áµ€Lâ‚€) âŠ— (Gáµ€GGáµ€G)
        J0 = kron(dot(SUL0.T, SUdC0US, SUL0).T, GGGG)
        # Xáµ€ğ“¡X = (Lâ‚€áµ€Uâ‚Sâ‚â»Â¹Uâ‚áµ€âˆ‚Câ‚Uâ‚Sâ‚â»Â¹Uâ‚áµ€Lâ‚€) âŠ— (Gáµ€G)
        J1 = kron(dot(SUL0.T, SUdC1US, SUL0).T, GG)

        # Xáµ€ğ“¡XZâ»Â¹Xáµ€Râ»Â¹ğ² over Câ‚€ parameters
        J0ZiXRiy = J0 @ ZiXRiy
        # Xáµ€ğ“¡XZâ»Â¹Xáµ€Râ»Â¹ğ¦ over Câ‚€ parameters
        J0ZiXRim = J0 @ ZiXRim
        # Xáµ€ğ“¡XZâ»Â¹Xáµ€Râ»Â¹ğ² over Câ‚ parameters
        J1ZiXRiy = J1 @ ZiXRiy
        # Xáµ€ğ“¡XZâ»Â¹Xáµ€Râ»Â¹ğ¦ over Câ‚ parameters
        J1ZiXRim = J1 @ ZiXRim

        GYC1idC0US = dot(GYUS, SUdC0US)
        # ğ²áµ€ğ“¡X over Câ‚€ parameters
        yR0X = vec(dot(GG, GYC1idC0US, SUL0))
        GmC1idC0US = dot(GmUS, SUdC0US)
        # ğ¦áµ€ğ“¡X over Câ‚€ parameters
        mR0X = vec(dot(GG, GmC1idC0US, SUL0))

        YC1idC1US = dot(YUS, SUdC1US)
        # ğ²áµ€ğ“¡X over Câ‚ parameters
        yR1X = vec(dot(Ge.T, YC1idC1US, SUL0))
        mC1idC1US = dot(mUS, SUdC1US)
        # ğ¦áµ€ğ“¡X over Câ‚ parameters
        mR1X = vec(dot(Ge.T, mC1idC1US, SUL0))

        # ğ²áµ€ğ“¡ğ² over Câ‚€ parameters
        yR0y = _sum((GYC1idC0US.T * GYUS.T).T)
        # ğ²áµ€ğ•‚ğ² over Câ‚€ parameters
        yKidKKiy = yR0y - 2 * yR0X.T @ ZiXRiy + ZiXRiy @ J0ZiXRiy.T
        grad["C0.Lu"] += yKidKKiy

        # ğ²áµ€ğ“¡ğ² over Câ‚ parameters
        yR1y = _sum((YC1idC1US.T * YUS.T).T)
        # ğ²áµ€ğ•‚ğ² over Câ‚ parameters
        yKidKKiy = yR1y - 2 * yR1X.T @ ZiXRiy + ZiXRiy @ J1ZiXRiy.T
        grad["C1.Lu"] += yKidKKiy

        # ğ¦áµ€ğ“¡ğ¦ over Câ‚€ parameters
        mR0m = _sum((GmC1idC0US.T * GmUS.T).T)
        # ğ¦áµ€ğ•‚ğ¦ over Câ‚€ parameters
        mKidKKim = mR0m - 2 * mR0X.T @ ZiXRim + ZiXRim @ J0ZiXRim.T
        grad["C0.Lu"] += mKidKKim

        # ğ¦áµ€ğ“¡ğ¦ over Câ‚ parameters
        mR1m = _sum((mC1idC1US.T * mUS.T).T)
        # ğ¦áµ€ğ•‚ğ¦ over Câ‚ parameters
        mKidKKim = mR1m - 2 * mR1X.T @ ZiXRim + ZiXRim @ J1ZiXRim.T
        grad["C1.Lu"] += mKidKKim

        # ğ²áµ€ğ“¡ğ¦ over Câ‚€ parameters
        yR0m = _sum((GYC1idC0US.T * GmUS.T).T)
        # ğ²áµ€ğ•‚ğ¦ over Câ‚€ parameters
        yKidKKim = yR0m - yR0X.T @ ZiXRim - mR0X.T @ ZiXRiy + ZiXRiy @ J0ZiXRim.T
        grad["C0.Lu"] -= 2 * yKidKKim

        # ğ²áµ€ğ“¡ğ¦ over Câ‚ parameters
        yR1m = _sum((YC1idC1US.T * mUS.T).T)
        # ğ²áµ€ğ•‚ğ¦ over Câ‚ parameters
        yKidKKim = yR1m - yR1X.T @ ZiXRim - mR1X.T @ ZiXRiy + ZiXRiy @ J1ZiXRim.T
        grad["C1.Lu"] -= 2 * yKidKKim

        for var in varnames:
            grad[var] -= diagonal(solve(terms["H"], dH[var]), axis1=1, axis2=2).sum(1)
            grad[var] += 2 * (terms["yl"] - terms["ml"]).T @ dmh[var]
            grad[var] /= 2

        return grad

    def fit(self, verbose=True):
        """
        Maximise the marginal likelihood.

        Parameters
        ----------
        verbose : bool, optional
            ``True`` for progress output; ``False`` otherwise.
            Defaults to ``True``.
        """
        self._maximize(verbose=verbose)
