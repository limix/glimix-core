import warnings
from functools import reduce

from numpy import asarray, asfortranarray, kron, log, sqrt, tensordot, trace
from numpy.linalg import inv, matrix_rank, slogdet
from optimix import Function

from glimix_core._util import cached_property, log2pi, unvec, vec
from glimix_core.cov import Kron2SumCov
from glimix_core.mean import KronMean

from ._kron2sum_scan import KronFastScanner


class Kron2Sum(Function):
    """
    LMM for multi-traits fitted via maximum likelihood.

    This implementation follows the work published in [CA05]_.
    Let n, c, and p be the number of samples, covariates, and traits, respectively.
    The outcome variable Y is a nÃ—p matrix distributed according to::

        vec(Y) ~ N((A âŠ— X) vec(B), K = Câ‚€ âŠ— GGáµ€ + Câ‚ âŠ— I).

    A and X are design matrices of dimensions pÃ—p and nÃ—c provided by the user,
    where X is the usual matrix of covariates commonly used in single-trait models.
    B is a cÃ—p matrix of fixed-effect sizes per trait.
    G is a nÃ—r matrix provided by the user and I is a nÃ—n identity matrices.
    Câ‚€ and Câ‚ are both symmetric matrices of dimensions pÃ—p, for which Câ‚ is
    guaranteed by our implementation to be of full rank.
    The parameters of this model are the matrices B, Câ‚€, and Câ‚.

    For implementation purpose, we make use of the following definitions:

    - ğ›ƒ = vec(B)
    - M = A âŠ— X
    - H = Máµ€Kâ»Â¹M
    - Yâ‚“ = Lâ‚“Y
    - Yâ‚• = Yâ‚“Lâ‚•áµ€
    - Mâ‚“ = Lâ‚“X
    - Mâ‚• = (Lâ‚•A) âŠ— Mâ‚“
    - mâ‚• = Mâ‚•vec(B)

    where Lâ‚“ and Lâ‚• are defined in :class:`glimix_core.cov.Kron2SumCov`.

    References
    ----------
    .. [CA05] Casale, F. P., Rakitsch, B., Lippert, C., & Stegle, O. (2015). Efficient
       set tests for the genetic analysis of correlated traits. Nature methods, 12(8),
       755.
    """

    def __init__(self, Y, A, X, G, rank=1, restricted=False):
        """
        Constructor.

        Parameters
        ----------
        Y : (n, p) array_like
            Outcome matrix.
        A : (n, n) array_like
            Trait-by-trait design matrix.
        X : (n, c) array_like
            Covariates design matrix.
        G : (n, r) array_like
            Matrix G from the GGáµ€ term.
        rank : optional, int
            Maximum rank of matrix Câ‚€. Defaults to ``1``.
        """
        from numpy_sugar import is_all_finite

        Y = asfortranarray(Y, float)
        yrank = matrix_rank(Y)
        if Y.shape[1] > yrank:
            warnings.warn(
                f"Y is not full column rank: rank(Y)={yrank}. "
                + "Convergence might be problematic.",
                UserWarning,
            )

        A = asarray(A, float)
        X = asarray(X, float)
        Xrank = matrix_rank(X)
        if X.shape[1] > Xrank:
            warnings.warn(
                f"X is not full column rank: rank(X)={Xrank}. "
                + "Convergence might be problematic.",
                UserWarning,
            )
        G = asarray(G, float).copy()
        self._G_norm = max(G.min(), G.max())
        G /= self._G_norm

        if not is_all_finite(Y):
            raise ValueError("There are non-finite values in the outcome matrix.")

        if not is_all_finite(A):
            msg = "There are non-finite values in the trait-by-trait design matrix."
            raise ValueError(msg)

        if not is_all_finite(X):
            raise ValueError("There are non-finite values in the covariates matrix.")

        if not is_all_finite(G):
            raise ValueError("There are non-finite values in the G matrix.")

        self._Y = Y
        self._cov = Kron2SumCov(G, Y.shape[1], rank)
        self._cov.listen(self._parameters_update)
        self._mean = KronMean(A, X)
        self._cache = {"terms": None}
        self._restricted = restricted
        composite = [("C0", self._cov.C0), ("C1", self._cov.C1)]
        Function.__init__(self, "Kron2Sum", composite=composite)

        nparams = self._mean.nparams + self._cov.nparams
        if nparams > Y.size:
            msg = "The number of parameters is larger than the outcome size."
            msg += " Convergence is expected to be problematic."
            warnings.warn(msg, UserWarning)

    @property
    def beta_covariance(self):
        """
        Estimates the covariance-matrix of the optimal beta.

        Returns
        -------
        beta-covariance : ndarray
            (Máµ€Kâ»Â¹M)â»Â¹.

        References
        ----------
        .. Rencher, A. C., & Schaalje, G. B. (2008). Linear models in statistics. John
           Wiley & Sons.
        """
        H = self._terms["H"]
        return inv(H)

    def get_fast_scanner(self):
        """
        Return :class:`.FastScanner` for association scan.

        Returns
        -------
        :class:`.FastScanner`
            Instance of a class designed to perform very fast association scan.
        """
        terms = self._terms
        return KronFastScanner(self._Y, self._mean.A, self._mean.X, self._cov.Ge, terms)

    @property
    def A(self):
        """
        A from the equation ğ¦ = (A âŠ— X) vec(B).

        Returns
        -------
        A : ndarray
            A.
        """
        return self._mean.A

    @property
    def B(self):
        """
        Fixed-effect sizes B from ğ¦ = (A âŠ— X) vec(B).

        Returns
        -------
        fixed-effects : ndarray
            B from ğ¦ = (A âŠ— X) vec(B).
        """
        self._terms
        return asarray(self._mean.B, float)

    @property
    def beta(self):
        """
        Fixed-effect sizes ğ›ƒ = vec(B).

        Returns
        -------
        fixed-effects : ndarray
            ğ›ƒ from ğ›ƒ = vec(B).
        """
        return vec(self.B)

    @property
    def C0(self):
        """
        Câ‚€ from equation K = Câ‚€ âŠ— GGáµ€ + Câ‚ âŠ— I.

        Returns
        -------
        C0 : ndarray
            Câ‚€.
        """
        return self._cov.C0.value() / (self._G_norm ** 2)

    @property
    def C1(self):
        """
        Câ‚ from equation K = Câ‚€ âŠ— GGáµ€ + Câ‚ âŠ— I.

        Returns
        -------
        C1 : ndarray
            Câ‚.
        """
        return self._cov.C1.value()

    def mean(self):
        """
        Mean ğ¦ = (A âŠ— X) vec(B).

        Returns
        -------
        mean : ndarray
            ğ¦.
        """
        self._terms
        return self._mean.value()

    def covariance(self):
        """
        Covariance K = Câ‚€ âŠ— GGáµ€ + Câ‚ âŠ— I.

        Returns
        -------
        covariance : ndarray
            K.
        """
        return self._cov.value()

    @property
    def X(self):
        """
        X from equation M = (A âŠ— X).

        Returns
        -------
        X : ndarray
            X from M = (A âŠ— X).
        """
        return self._mean.X

    @property
    def M(self):
        """
        M = (A âŠ— X).

        Returns
        -------
        M : ndarray
            M from M = (A âŠ— X).
        """
        return self._mean.AX

    @property
    def nsamples(self):
        """
        Number of samples, n.
        """
        return self._Y.shape[0]

    @property
    def ntraits(self):
        """
        Number of traits, p.
        """
        return self._Y.shape[1]

    @property
    def ncovariates(self):
        """
        Number of covariates, c.
        """
        return self._mean.X.shape[1]

    def value(self):
        """
        Log of the marginal likelihood.
        """
        return self.lml()

    def gradient(self):
        """
        Gradient of the log of the marginal likelihood.
        """
        return self._lml_gradient()

    def lml(self):
        """
        Log of the marginal likelihood.

        Let ğ² = vec(Y), M = AâŠ—X, and H = Máµ€Kâ»Â¹M. The restricted log of the marginal
        likelihood is given by [R07]_::

            2â‹…log(p(ğ²)) = -(nâ‹…p - câ‹…p) log(2Ï€) + log(ï½œMáµ€Mï½œ) - log(ï½œKï½œ) - log(ï½œHï½œ)
                - (ğ²-ğ¦)áµ€ Kâ»Â¹ (ğ²-ğ¦),

        where ğ¦ = Mğ›ƒ for ğ›ƒ = Hâ»Â¹Máµ€Kâ»Â¹ğ².

        For implementation purpose, let X = (Lâ‚€ âŠ— G) and R = (Lâ‚ âŠ— I)(Lâ‚ âŠ— I)áµ€.
        The covariance can be written as::

            K = XXáµ€ + R.

        From the Woodbury matrix identity, we have

            ğ²áµ€Kâ»Â¹ğ² = ğ²áµ€Râ»Â¹ğ² - ğ²áµ€Râ»Â¹XZâ»Â¹Xáµ€Râ»Â¹ğ²,

        where Z = I + Xáµ€Râ»Â¹X. Note that Râ»Â¹ = (Uâ‚Sâ‚â»Â¹Uâ‚áµ€) âŠ— I and ::

            Xáµ€Râ»Â¹ğ² = (Lâ‚€áµ€W âŠ— Gáµ€)ğ² = vec(Gáµ€YWLâ‚€),

        where W = Uâ‚Sâ‚â»Â¹Uâ‚áµ€. The term Gáµ€Y can be calculated only once and it will form a
        rÃ—p matrix. We similarly have ::

            Xáµ€Râ»Â¹M = (Lâ‚€áµ€WA) âŠ— (Gáµ€X),

        for which Gáµ€X is pre-computed.

        The log-determinant of the covariance matrix is given by

            log(ï½œKï½œ) = log(ï½œZï½œ) - log(ï½œRâ»Â¹ï½œ) = log(ï½œZï½œ) - 2Â·nÂ·log(ï½œUâ‚Sâ‚â»Â½ï½œ).

        The log of the marginal likelihood can be rewritten as::

            2â‹…log(p(ğ²)) = -(nâ‹…p - câ‹…p) log(2Ï€) + log(ï½œMáµ€Mï½œ)
            - log(ï½œZï½œ) + 2Â·nÂ·log(ï½œUâ‚Sâ‚â»Â½ï½œ)
            - log(ï½œMáµ€Râ»Â¹M - Máµ€Râ»Â¹XZâ»Â¹Xáµ€Râ»Â¹Mï½œ)
            - ğ²áµ€Râ»Â¹ğ² + (ğ²áµ€Râ»Â¹X)Zâ»Â¹(Xáµ€Râ»Â¹ğ²)
            - ğ¦áµ€Râ»Â¹ğ¦ + (ğ¦áµ€Râ»Â¹X)Zâ»Â¹(Xáµ€Râ»Â¹ğ¦)
            + 2ğ²áµ€Râ»Â¹ğ¦ - 2(ğ²áµ€Râ»Â¹X)Zâ»Â¹(Xáµ€Râ»Â¹ğ¦).

        Returns
        -------
        lml : float
            Log of the marginal likelihood.

        References
        ----------
        .. [R07] LaMotte, L. R. (2007). A direct derivation of the REML likelihood
           function. Statistical Papers, 48(2), 321-327.
        """
        terms = self._terms
        yKiy = terms["yKiy"]
        mKiy = terms["mKiy"]
        mKim = terms["mKim"]

        lml = -self._df * log2pi + self._logdet_MM - self._logdetK
        lml -= self._logdetH
        lml += -yKiy - mKim + 2 * mKiy

        return lml / 2

    def fit(self, verbose=True):
        """
        Maximise the marginal likelihood.

        Parameters
        ----------
        verbose : bool, optional
            ``True`` for progress output; ``False`` otherwise.
            Defaults to ``True``.
        """
        self._maximize(verbose=verbose, factr=1e7, pgtol=1e-7)

    def _parameters_update(self):
        self._cache["terms"] = None

    @cached_property
    def _GY(self):
        return self._cov.Ge.T @ self._Y

    @cached_property
    def _GG(self):
        return self._cov.Ge.T @ self._cov.Ge

    @cached_property
    def _trGG(self):
        from numpy_sugar.linalg import trace2

        return trace2(self._cov.Ge, self._cov.Ge.T)

    @cached_property
    def _GGGG(self):
        return self._GG @ self._GG

    @cached_property
    def _GGGY(self):
        return self._GG @ self._GY

    @cached_property
    def _XX(self):
        return self._mean.X.T @ self._mean.X

    @cached_property
    def _GX(self):
        return self._cov.Ge.T @ self._mean.X

    @cached_property
    def _XGGG(self):
        return self._GX.T @ self._GG

    @cached_property
    def _XGGY(self):
        return self._GX.T @ self._GY

    @cached_property
    def _XGGX(self):
        return self._GX.T @ self._GX

    @cached_property
    def _XY(self):
        return self._mean.X.T @ self._Y

    @property
    def _terms(self):
        from numpy_sugar.linalg import ddot, sum2diag
        from scipy.linalg import cho_factor, cho_solve

        if self._cache["terms"] is not None:
            return self._cache["terms"]

        L0 = self._cov.C0.L
        S, U = self._cov.C1.eigh()
        W = ddot(U, 1 / S) @ U.T
        S = 1 / sqrt(S)
        Y = self._Y
        A = self._mean.A

        WL0 = W @ L0
        YW = Y @ W
        WA = W @ A
        L0WA = L0.T @ WA

        Z = kron(L0.T @ WL0, self._GG)
        Z = sum2diag(Z, 1)
        Lz = cho_factor(Z, lower=True)

        # ğ²áµ€Râ»Â¹ğ² = vec(YW)áµ€ğ²
        yRiy = (YW * self._Y).sum()
        # Máµ€Râ»Â¹M = Aáµ€WA âŠ— Xáµ€X
        MRiM = kron(A.T @ WA, self._XX)
        # Xáµ€Râ»Â¹ğ² = vec(Gáµ€YWLâ‚€)
        XRiy = vec(self._GY @ WL0)
        # Xáµ€Râ»Â¹M = (Lâ‚€áµ€WA) âŠ— (Gáµ€X)
        XRiM = kron(L0WA, self._GX)
        # Máµ€Râ»Â¹ğ² = vec(Xáµ€YWA)
        MRiy = vec(self._XY @ WA)

        ZiXRiM = cho_solve(Lz, XRiM)
        ZiXRiy = cho_solve(Lz, XRiy)

        MRiXZiXRiy = ZiXRiM.T @ XRiy
        MRiXZiXRiM = XRiM.T @ ZiXRiM

        yKiy = yRiy - XRiy @ ZiXRiy
        MKiy = MRiy - MRiXZiXRiy
        H = MRiM - MRiXZiXRiM
        Lh = cho_factor(H)
        b = cho_solve(Lh, MKiy)
        B = unvec(b, (self.ncovariates, -1))
        self._mean.B = B
        XRim = XRiM @ b

        ZiXRim = ZiXRiM @ b
        mRiy = b.T @ MRiy
        mRim = b.T @ MRiM @ b

        logdetK = log(Lz[0].diagonal()).sum() * 2
        logdetK -= 2 * log(S).sum() * self.nsamples

        mKiy = mRiy - XRim.T @ ZiXRiy
        mKim = mRim - XRim.T @ ZiXRim

        self._cache["terms"] = {
            "logdetK": logdetK,
            "mKiy": mKiy,
            "mKim": mKim,
            "b": b,
            "Z": Z,
            "B": B,
            "Lz": Lz,
            "S": S,
            "W": W,
            "WA": WA,
            "YW": YW,
            "WL0": WL0,
            "yRiy": yRiy,
            "MRiM": MRiM,
            "XRiy": XRiy,
            "XRiM": XRiM,
            "ZiXRiM": ZiXRiM,
            "ZiXRiy": ZiXRiy,
            "ZiXRim": ZiXRim,
            "MRiy": MRiy,
            "mRim": mRim,
            "mRiy": mRiy,
            "XRim": XRim,
            "yKiy": yKiy,
            "H": H,
            "Lh": Lh,
            "MRiXZiXRiy": MRiXZiXRiy,
            "MRiXZiXRiM": MRiXZiXRiM,
        }
        return self._cache["terms"]

    def _lml_gradient(self):
        """
        Gradient of the log of the marginal likelihood.

        Let ğ² = vec(Y), ğ•‚ = Kâ»Â¹âˆ‚(K)Kâ»Â¹, and H = Máµ€Kâ»Â¹M. The gradient is given by::

            2â‹…âˆ‚log(p(ğ²)) = -tr(Kâ»Â¹âˆ‚K) - tr(Hâ»Â¹âˆ‚H) + ğ²áµ€ğ•‚ğ² - ğ¦áµ€ğ•‚(2â‹…ğ²-ğ¦)
                - 2â‹…(ğ¦-ğ²)áµ€Kâ»Â¹âˆ‚(ğ¦).

        Observe that

            âˆ‚ğ›ƒ = -Hâ»Â¹(âˆ‚H)ğ›ƒ - Hâ»Â¹Máµ€ğ•‚ğ² and âˆ‚H = -Máµ€ğ•‚M.

        Let Z = I + Xáµ€Râ»Â¹X and ğ“¡ = Râ»Â¹(âˆ‚K)Râ»Â¹. We use Woodbury matrix identity to
        write ::

            ğ²áµ€ğ•‚ğ² = ğ²áµ€ğ“¡ğ² - 2(ğ²áµ€ğ“¡X)Zâ»Â¹(Xáµ€Râ»Â¹ğ²) + (ğ²áµ€Râ»Â¹X)Zâ»Â¹(Xáµ€ğ“¡X)Zâ»Â¹(Xáµ€Râ»Â¹ğ²)
            Máµ€ğ•‚M = Máµ€ğ“¡M - 2(Máµ€ğ“¡X)Zâ»Â¹(Xáµ€Râ»Â¹M) + (Máµ€Râ»Â¹X)Zâ»Â¹(Xáµ€ğ“¡X)Zâ»Â¹(Xáµ€Râ»Â¹M)
            Máµ€ğ•‚ğ² = Máµ€ğ“¡ğ² - (Máµ€Râ»Â¹X)Zâ»Â¹(Xáµ€ğ“¡ğ²) - (Máµ€ğ“¡X)Zâ»Â¹(Xáµ€Râ»Â¹ğ²)
                  + (Máµ€Râ»Â¹X)Zâ»Â¹(Xáµ€ğ“¡X)Zâ»Â¹(Xáµ€Râ»Â¹ğ²)
            Hâ»Â¹   = Máµ€Râ»Â¹M - (Máµ€Râ»Â¹X)Zâ»Â¹(Xáµ€Râ»Â¹M),

        where we have used parentheses to separate expressions
        that we will compute separately. For example, we have ::

            ğ²áµ€ğ“¡ğ² = ğ²áµ€(Uâ‚Sâ‚â»Â¹Uâ‚áµ€ âŠ— I)(âˆ‚Câ‚€ âŠ— GGáµ€)(Uâ‚Sâ‚â»Â¹Uâ‚áµ€ âŠ— I)ğ²
                  = ğ²áµ€(Uâ‚Sâ‚â»Â¹Uâ‚áµ€âˆ‚Câ‚€ âŠ— G)(Uâ‚Sâ‚â»Â¹Uâ‚áµ€ âŠ— Gáµ€)ğ²
                  = vec(Gáµ€YUâ‚Sâ‚â»Â¹Uâ‚áµ€âˆ‚Câ‚€)áµ€vec(Gáµ€YUâ‚Sâ‚â»Â¹Uâ‚áµ€),

        when the derivative is over the parameters of Câ‚€. Otherwise, we have

            ğ²áµ€ğ“¡ğ² = vec(YUâ‚Sâ‚â»Â¹Uâ‚áµ€âˆ‚Câ‚)áµ€vec(YUâ‚Sâ‚â»Â¹Uâ‚áµ€).

        The above equations can be more compactly written as

            ğ²áµ€ğ“¡ğ² = vec(Eáµ¢áµ€YWâˆ‚Cáµ¢)áµ€vec(Eáµ¢áµ€YW),

        where W = Uâ‚Sâ‚â»Â¹Uâ‚áµ€, Eâ‚€ = G, and Eâ‚ = I. We will now just state the results for
        the other instances of the aBc form, which follow similar derivations::

            Xáµ€ğ“¡X = (Lâ‚€áµ€Wâˆ‚Cáµ¢WLâ‚€) âŠ— (Gáµ€Eáµ¢Eáµ¢áµ€G)
            Máµ€ğ“¡y = (Aáµ€Wâˆ‚Cáµ¢âŠ—Xáµ€Eáµ¢)vec(Eáµ¢áµ€YW) = vec(Xáµ€Eáµ¢Eáµ¢áµ€YWâˆ‚Cáµ¢WA)
            Máµ€ğ“¡X = Aáµ€Wâˆ‚Cáµ¢WLâ‚€ âŠ— Xáµ€Eáµ¢Eáµ¢áµ€G
            Máµ€ğ“¡M = Aáµ€Wâˆ‚Cáµ¢WA âŠ— Xáµ€Eáµ¢Eáµ¢áµ€X
            Xáµ€ğ“¡ğ² = Gáµ€Eáµ¢Eáµ¢áµ€YWâˆ‚Cáµ¢WLâ‚€

        From Woodbury matrix identity and Kronecker product properties we have ::

            tr(Kâ»Â¹âˆ‚K) = tr[Wâˆ‚Cáµ¢]tr[Eáµ¢Eáµ¢áµ€] - tr[Zâ»Â¹(Xáµ€ğ“¡X)]
            tr(Hâ»Â¹âˆ‚H) = - tr[(Máµ€Râ»Â¹M)(Máµ€ğ•‚M)] + tr[(Máµ€Râ»Â¹X)Zâ»Â¹(Xáµ€Râ»Â¹M)(Máµ€ğ•‚M)]

        Note also that ::

            âˆ‚ğ›ƒ = Hâ»Â¹Máµ€ğ•‚Mğ›ƒ - Hâ»Â¹Máµ€ğ•‚ğ².

        Returns
        -------
        C0.Lu : ndarray
            Gradient of the log of the marginal likelihood over Câ‚€ parameters.
        C1.Lu : ndarray
            Gradient of the log of the marginal likelihood over Câ‚ parameters.
        """
        from scipy.linalg import cho_solve

        terms = self._terms
        dC0 = self._cov.C0.gradient()["Lu"]
        dC1 = self._cov.C1.gradient()["Lu"]

        b = terms["b"]
        W = terms["W"]
        Lh = terms["Lh"]
        Lz = terms["Lz"]
        WA = terms["WA"]
        WL0 = terms["WL0"]
        YW = terms["YW"]
        MRiM = terms["MRiM"]
        MRiy = terms["MRiy"]
        XRiM = terms["XRiM"]
        XRiy = terms["XRiy"]
        ZiXRiM = terms["ZiXRiM"]
        ZiXRiy = terms["ZiXRiy"]

        WdC0 = _mdot(W, dC0)
        WdC1 = _mdot(W, dC1)

        AWdC0 = _mdot(WA.T, dC0)
        AWdC1 = _mdot(WA.T, dC1)

        # Máµ€ğ“¡M
        MR0M = _mkron(_mdot(AWdC0, WA), self._XGGX)
        MR1M = _mkron(_mdot(AWdC1, WA), self._XX)

        # Máµ€ğ“¡X
        MR0X = _mkron(_mdot(AWdC0, WL0), self._XGGG)
        MR1X = _mkron(_mdot(AWdC1, WL0), self._GX.T)

        # Máµ€ğ“¡ğ² = (Aáµ€Wâˆ‚Cáµ¢âŠ—Xáµ€Eáµ¢)vec(Eáµ¢áµ€YW) = vec(Xáµ€Eáµ¢Eáµ¢áµ€YWâˆ‚Cáµ¢WA)
        MR0y = vec(_mdot(self._XGGY, _mdot(WdC0, WA)))
        MR1y = vec(_mdot(self._XY, WdC1, WA))

        # Xáµ€ğ“¡X
        XR0X = _mkron(_mdot(WL0.T, dC0, WL0), self._GGGG)
        XR1X = _mkron(_mdot(WL0.T, dC1, WL0), self._GG)

        # Xáµ€ğ“¡ğ²
        XR0y = vec(_mdot(self._GGGY, WdC0, WL0))
        XR1y = vec(_mdot(self._GY, WdC1, WL0))

        # ğ²áµ€ğ“¡ğ² = vec(Eáµ¢áµ€YWâˆ‚Cáµ¢)áµ€vec(Eáµ¢áµ€YW)
        yR0y = vec(_mdot(self._GY, WdC0)).T @ vec(self._GY @ W)
        yR1y = (YW.T * _mdot(self._Y, WdC1).T).T.sum(axis=(0, 1))

        ZiXR0X = cho_solve(Lz, XR0X)
        ZiXR1X = cho_solve(Lz, XR1X)
        ZiXR0y = cho_solve(Lz, XR0y)
        ZiXR1y = cho_solve(Lz, XR1y)

        # Máµ€ğ•‚y = Máµ€ğ“¡ğ² - (Máµ€Râ»Â¹X)Zâ»Â¹(Xáµ€ğ“¡ğ²) - (Máµ€ğ“¡X)Zâ»Â¹(Xáµ€Râ»Â¹ğ²)
        #       + (Máµ€Râ»Â¹X)Zâ»Â¹(Xáµ€ğ“¡X)Zâ»Â¹(Xáµ€Râ»Â¹ğ²)
        MK0y = MR0y - _mdot(XRiM.T, ZiXR0y) - _mdot(MR0X, ZiXRiy)
        MK0y += _mdot(XRiM.T, ZiXR0X, ZiXRiy)
        MK1y = MR1y - _mdot(XRiM.T, ZiXR1y) - _mdot(MR1X, ZiXRiy)
        MK1y += _mdot(XRiM.T, ZiXR1X, ZiXRiy)

        # ğ²áµ€ğ•‚ğ² = ğ²áµ€ğ“¡ğ² - 2(ğ²áµ€ğ“¡X)Zâ»Â¹(Xáµ€Râ»Â¹ğ²) + (ğ²áµ€Râ»Â¹X)Zâ»Â¹(Xáµ€ğ“¡X)Zâ»Â¹(Xáµ€Râ»Â¹ğ²)
        yK0y = yR0y - 2 * XR0y.T @ ZiXRiy + ZiXRiy.T @ _mdot(XR0X, ZiXRiy)
        yK1y = yR1y - 2 * XR1y.T @ ZiXRiy + ZiXRiy.T @ _mdot(XR1X, ZiXRiy)

        # Máµ€ğ•‚M = Máµ€ğ“¡M - (Máµ€ğ“¡X)Zâ»Â¹(Xáµ€Râ»Â¹M) - (Máµ€Râ»Â¹X)Zâ»Â¹(Xáµ€ğ“¡M)
        #       + (Máµ€Râ»Â¹X)Zâ»Â¹(Xáµ€ğ“¡X)Zâ»Â¹(Xáµ€Râ»Â¹M)
        MR0XZiXRiM = _mdot(MR0X, ZiXRiM)
        MK0M = MR0M - MR0XZiXRiM - MR0XZiXRiM.transpose([1, 0, 2])
        MK0M += _mdot(ZiXRiM.T, XR0X, ZiXRiM)
        MR1XZiXRiM = _mdot(MR1X, ZiXRiM)
        MK1M = MR1M - MR1XZiXRiM - MR1XZiXRiM.transpose([1, 0, 2])
        MK1M += _mdot(ZiXRiM.T, XR1X, ZiXRiM)

        MK0m = _mdot(MK0M, b)
        mK0y = b.T @ MK0y
        mK0m = b.T @ MK0m
        MK1m = _mdot(MK1M, b)
        mK1y = b.T @ MK1y
        mK1m = b.T @ MK1m
        XRim = XRiM @ b
        MRim = MRiM @ b

        db = {"C0.Lu": cho_solve(Lh, MK0m - MK0y), "C1.Lu": cho_solve(Lh, MK1m - MK1y)}

        grad = {
            "C0.Lu": -trace(WdC0) * self._trGG + trace(ZiXR0X),
            "C1.Lu": -trace(WdC1) * self.nsamples + trace(ZiXR1X),
        }

        if self._restricted:
            grad["C0.Lu"] += cho_solve(Lh, MK0M).diagonal().sum(1)
            grad["C1.Lu"] += cho_solve(Lh, MK1M).diagonal().sum(1)

        mKiM = MRim.T - XRim.T @ ZiXRiM
        yKiM = MRiy.T - XRiy.T @ ZiXRiM

        grad["C0.Lu"] += yK0y - 2 * mK0y + mK0m - 2 * _mdot(mKiM, db["C0.Lu"])
        grad["C0.Lu"] += 2 * _mdot(yKiM, db["C0.Lu"])
        grad["C1.Lu"] += yK1y - 2 * mK1y + mK1m - 2 * _mdot(mKiM, db["C1.Lu"])
        grad["C1.Lu"] += 2 * _mdot(yKiM, db["C1.Lu"])

        grad["C0.Lu"] /= 2
        grad["C1.Lu"] /= 2

        return grad

    @cached_property
    def _logdet_MM(self):
        if not self._restricted:
            return 0.0

        M = self._mean.AX
        ldet = slogdet(M.T @ M)
        if ldet[0] != 1.0:
            raise ValueError("The determinant of Máµ€M should be positive.")
        return ldet[1]

    @property
    def _logdetH(self):
        if not self._restricted:
            return 0.0
        terms = self._terms
        MKiM = terms["MRiM"] - terms["XRiM"].T @ terms["ZiXRiM"]
        return slogdet(MKiM)[1]

    @property
    def _logdetK(self):
        terms = self._terms
        S = terms["S"]
        Lz = terms["Lz"]

        cov_logdet = log(Lz[0].diagonal()).sum() * 2
        cov_logdet -= 2 * log(S).sum() * self.nsamples
        return cov_logdet

    @property
    def _df(self):
        np = self.nsamples * self.ntraits
        if not self._restricted:
            return np
        cp = self.ncovariates * self.ntraits
        return np - cp


def _dot(a, b):
    r = tensordot(a, b, axes=([min(1, a.ndim - 1)], [0]))
    if a.ndim > b.ndim:
        if r.ndim == 3:
            return r.transpose([0, 2, 1])
        return r
    return r


def _mdot(*args):
    return reduce(_dot, args)


def _mkron(a, b):
    if a.ndim == 3:
        return kron(a.transpose([2, 0, 1]), b).transpose([1, 2, 0])
    return kron(a, b)
